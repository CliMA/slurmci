# analyze_perf.jl
#
# Parse the logs generated by nvprof for each performance test and
# summarize results across ranks.

using DataFrames, OrderedCollections, UnicodePlots

# sha (commit hash) | fun | timepercent | time | avgncalls | avgd | mind | maxd | nranks
#
# sha (commit hash) | fun | mname | mavg | mmin | mmax | nranks

function parse_sumlog!(pdf::DataFrame, sha::String, filename::String)
    # load and parse out all "GPU activities" lines
    lns = readlines(filename)
    for i = 1:length(lns)
        lns[i][1] != '\"' && continue
        wds = split(lns[i], ",")
        wds[1] != "\"GPU activities\"" && continue
        onm = strip(wds[8], '"')
        r = findfirst("ptxcall_", onm)
        s = r == nothing ? 1 : r.stop+1
        fun = onm[s:end]
        # sha, fun, time%, time, ncalls, avg, min, max, nranks
        push!(pdf, (sha, fun, parse(Float64, wds[2]), parse(Float64, wds[3]),
                    parse(Int64, wds[4]), parse(Float64, wds[5]),
                    parse(Float64, wds[6]), parse(Float64, wds[7]), 1))
    end
end

function parse_metlog!(mdf::DataFrame, sha::String, filename::String)
    # load and parse out all metrics lines
    lns = readlines(filename)

    # skip over the header lines
    j = 0
    while true
        j += 1
        if j > length(lns) || (lns[j][1] == '\"' && lns[j][2:7] == "Device")
            break
        end
    end
    for i = j+1:length(lns)
        wds = split(lns[i], ",")
        mmin, mmax, mavg = parse(Int64, wds[6]), parse(Int64, wds[7]), parse(Int64, wds[8])
        mmin == 0 && continue
        onm = strip(wds[2], '\"')
        r = findfirst("ptxcall_", onm)
        s = r == nothing ? 1 : r.stop+1
        fun = onm[s:end]
        mname = strip(wds[4], '\"')
        # sha, fun, metricname, avg, min, max, nranks
        push!(mdf, (sha, fun, mname, mavg, mmin, mmax, 1))
    end
end

function condense_summaries!(summaries::Dict{String,DataFrame}, sha::String)
    for testname in keys(summaries)
        pdf = summaries[testname]
        newpdf = similar(pdf, 0)
        funnames = unique(pdf[!,:fun])
        for funname in funnames
            fdf = pdf[pdf[!,:fun] .== funname, :]

            nranks = nrow(fdf)
            timepercent = sum(fdf[!, :timepercent]) / nranks
            time = sum(fdf[!, :time]) / nranks
            avgncalls = sum(fdf[!, :avgncalls]) / nranks
            avgd = sum(fdf[!, :avgd]) / nranks
            mind = minimum(fdf[!, :mind])
            maxd = maximum(fdf[!, :maxd])
            push!(newpdf, (sha, funname, timepercent, time, avgncalls,
                           avgd, mind, maxd, nranks))
        end
        summaries[testname] = newpdf
    end
end

function condense_metrics!(metrics::Dict{String,DataFrame}, sha::String)
    for testname in keys(metrics)
        mdf = metrics[testname]
        newmdf = similar(mdf, 0)
        funnames = unique(mdf[!,:fun])
        for funname in funnames
            fdf = mdf[mdf[!,:fun] .== funname, :]

            mnames = unique(fdf[!,:mname])
            for mname in mnames
                mmdf = fdf[fdf[!,:mname] .== mname, :]

                mmin = minimum(mmdf[!, :mmin])
                mmax = maximum(mmdf[!, :mmax])
                nranks = nrow(mmdf)
                mavg = div(sum(mmdf[!, :mavg]), nranks)
                push!(newmdf, (sha, funname, mname, mavg, mmin, mmax, nranks))
            end
        end
        metrics[testname] = newmdf
    end
end

function analyze_perf(sha::String)
    dir = joinpath(logdir, sha)
    pfiles = filter(f -> length(f) > 15, readdir(dir))

    # first parse "*.summary.nvplog" in `dir`
    summaries = Dict{String,DataFrame}()
    sumlogs = filter(f -> f[end-14:end] == ".summary.nvplog", pfiles)
    for i = 1:length(sumlogs)
        r = findfirst(".jl", sumlogs[i])
        r == nothing && continue
        testname = sumlogs[i][1:r.start-1]
        pdf = DataFrame(sha = String[], fun = String[], timepercent = Float64[],
                        time = Float64[], avgncalls = Int64[],
                        avgd = Float64[], mind = Float64[], maxd = Float64[],
                        nranks = Int[])
        parse_sumlog!(pdf, sha, joinpath(dir, sumlogs[i]))
        if haskey(summaries, testname)
            pdf = append!(summaries[testname], pdf)
        end
        summaries[testname] = pdf
    end

    # now parse "*.metrics.nvplog" in `dir`
    metrics = Dict{String,DataFrame}()
    metlogs = filter(f -> f[end-14:end] == ".metrics.nvplog", pfiles)
    for i = 1:length(metlogs)
        r = findfirst(".jl", metlogs[i])
        r == nothing && continue
        testname = metlogs[i][1:r.start-1]
        mdf = DataFrame(sha = String[], fun = String[], mname = String[],
                        mavg = Int64[], mmin = Int64[], mmax = Int64[],
                        nranks = Int[])
        parse_metlog!(mdf, sha, joinpath(dir, metlogs[i]))
        if haskey(metrics, testname)
            mdf = append!(metrics[testname], mdf)
        end
        metrics[testname] = mdf
    end

    # summarize
    condense_summaries!(summaries, sha)
    condense_metrics!(metrics, sha)

    return summaries, metrics
end

